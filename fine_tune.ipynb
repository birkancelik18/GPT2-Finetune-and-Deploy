{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/novus/miniconda3/envs/novus/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "# us the model\n",
    "from transformers import pipeline, set_seed\n",
    "import wandb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Tokenize\n",
    "\n",
    "***Model*** The GPT2 Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n",
    "\n",
    "**Tokenizer**:  A tokenizer is in charge of preparing the inputs for a model.\n",
    "\n",
    "***PreTrainedTokenizer*** and ***PreTrainedTokenizerFast*** thus implement the main methods for using all the tokenizers:\n",
    "\n",
    "- Tokenizing (splitting strings in sub-word token strings), converting tokens strings to ids and back, and encoding/decoding (i.e., tokenizing and converting to integers).\n",
    "- Adding new tokens to the vocabulary in a way that is independent of the underlying structure (BPE, SentencePiece‚Ä¶).\n",
    "- Managing special tokens (like mask, beginning-of-sentence, etc.): adding them, assigning them to attributes in the tokenizer for easy access and making sure they are not split during tokenization. \n",
    "    \n",
    "Here is the link to [documentation](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)\n",
    "\n",
    "\n",
    "- GPT-2 Small ('gpt2'): 124 million parameters.\n",
    "- GPT-2 Medium ('gpt2-medium'): 345 million parameters.\n",
    "- GPT-2 Large ('gpt2-large'): 774 million parameters.\n",
    "- GPT-2 XL ('gpt2-xl'): 1.5 billion parameters.\n",
    "\n",
    "***Byte-Pair Encoding (BPE)*** vs ***Word Level Encoding***\n",
    "\n",
    "BPE emphasises more on subwords. Yet there might be issues with semantic information of those subwords. \n",
    "Word Level Encoding encodes word by word that preserves the semantic information more yet it has problems with unseen word encoding etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Data collators*** are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of train_dataset or eval_dataset.\n",
    "\n",
    "To be able to build batches, data collators may apply some processing (like padding). Some of them (like DataCollatorForLanguageModeling) also apply some random data augmentation (like random masking) on the formed batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/novus/miniconda3/envs/novus/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load your Shakespeare dataset\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"shakespeare_dataset.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "tokenizer (PreTrainedTokenizer or PreTrainedTokenizerFast) ‚Äî The tokenizer used for encoding the data.\n",
    "\n",
    "mlm (bool, optional, defaults to True) ‚Äî Whether or not to use masked language modeling.\n",
    " If set to False, the labels are the same as the inputs with the padding tokens ignored (by setting them to -100). \n",
    " Otherwise, the labels are -100 for non-masked tokens and the value to predict for the masked token.\n",
    "\n",
    "mlm_probability (float, optional, defaults to 0.15) ‚Äî The probability with which to (randomly) mask tokens in the input, when mlm is set to True.\n",
    "\n",
    "pad_to_multiple_of (int, optional) ‚Äî If set will pad the sequence to a multiple of the provided value.\n",
    "\n",
    "return_tensors (str) ‚Äî The type of Tensor to return. Allowable values are ‚Äúnp‚Äù, ‚Äúpt‚Äù and ‚Äútf‚Äù.\n",
    "\"\"\"\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # No masked language modeling for GPT-2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,    11,\n",
       "         3285,   502,  2740,    13,   198,   198,  3237,    25,   198,  5248,\n",
       "          461,    11,  2740,    13,   198,   198,  5962, 22307,    25,   198,\n",
       "         1639,   389,   477, 12939,  2138,   284,  4656,   621,   284,  1145,\n",
       "          680,    30,   198,   198,  3237,    25,   198,  4965,  5634,    13,\n",
       "        12939,    13,   198,   198,  5962, 22307,    25,   198,  5962,    11,\n",
       "          345,   760,   327,  1872,   385,  1526, 28599,   318,  4039,  4472,\n",
       "          284,   262,   661,    13,   198,   198,  3237,    25,   198,  1135,\n",
       "          760,   470,    11,   356,   760,   470,    13,   198,   198,  5962,\n",
       "        22307,    25,   198,  5756,   514,  1494,   683,    11,   290,   356,\n",
       "         1183,   423, 11676,   379,   674,   898,  2756,    13,   198,  3792,\n",
       "          470,   257, 15593,    30,   198,   198,  3237,    25,   198,  2949,\n",
       "          517,  3375,   319,   470,    26,  1309,   340,   307])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the example notebook from Hugging Face about finetuning a model. [Notebook Link](https://github.com/huggingface/notebooks/blob/main/examples/summarization.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbirkan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/novus/Desktop/model_finetune_deploy/wandb/run-20240208_101418-ik1aw7yz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/birkan/model_finetune_deploy/runs/ik1aw7yz' target=\"_blank\">rose-voice-7</a></strong> to <a href='https://wandb.ai/birkan/model_finetune_deploy' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/birkan/model_finetune_deploy' target=\"_blank\">https://wandb.ai/birkan/model_finetune_deploy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/birkan/model_finetune_deploy/runs/ik1aw7yz' target=\"_blank\">https://wandb.ai/birkan/model_finetune_deploy/runs/ik1aw7yz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine-tuned-shakespeare\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,  # Adjust the number of epochs based on your needs\n",
    "    per_device_train_batch_size=4,  # Adjust batch size based on GPU memory\n",
    "    save_steps=10_000,  # Adjust save steps based on your needs\n",
    ")\n",
    "\n",
    "\n",
    "wandb.init(config=training_args)\n",
    "# Magic\n",
    "wandb.watch(model, log_freq=2)\n",
    "\n",
    "\n",
    "# Create Trainer and fine-tune the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|‚ñâ         | 613/6600 [14:17<2:19:31,  1.40s/it]\n",
      "\n",
      "\u001b[A                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3508, 'learning_rate': 4.621212121212121e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2187, 'learning_rate': 4.242424242424243e-05, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1049, 'learning_rate': 3.8636363636363636e-05, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9811, 'learning_rate': 3.484848484848485e-05, 'epoch': 3.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8553, 'learning_rate': 3.106060606060606e-05, 'epoch': 3.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7834, 'learning_rate': 2.7272727272727273e-05, 'epoch': 4.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6924, 'learning_rate': 2.3484848484848487e-05, 'epoch': 5.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6635, 'learning_rate': 1.9696969696969697e-05, 'epoch': 6.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5593, 'learning_rate': 1.590909090909091e-05, 'epoch': 6.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5256, 'learning_rate': 1.2121212121212122e-05, 'epoch': 7.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4717, 'learning_rate': 8.333333333333334e-06, 'epoch': 8.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4566, 'learning_rate': 4.5454545454545455e-06, 'epoch': 9.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4181, 'learning_rate': 7.575757575757576e-07, 'epoch': 9.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6600/6600 [59:32<00:00,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 3572.4447, 'train_samples_per_second': 7.39, 'train_steps_per_second': 1.847, 'train_loss': 2.7702815061627013, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6600, training_loss=2.7702815061627013, metrics={'train_runtime': 3572.4447, 'train_samples_per_second': 7.39, 'train_steps_per_second': 1.847, 'train_loss': 2.7702815061627013, 'epoch': 10.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() # report to wights to biases \n",
    "                                     # wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "    \n",
    "set_seed(42)\n",
    "\n",
    "response_model = generator(\"Before we proceed any further, hear me speak,\", max_length=200, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak,\n",
      "Myself having well begun and my mind's fury\n",
      "Not so inclined to encounter and take\n",
      "Exceeding pleasure in its own volition,\n",
      "My throat was then so mute and open\n",
      "As from a story, or my body so mute\n",
      "I had not yet been instructed to speak.\n",
      "For what satisfaction may I speak?\n",
      "It cannot be expostulated\n",
      "In my mind, for the end of my speech\n",
      "Was not thought of.\n",
      "\n",
      "DUKE OF YORK:\n",
      "Come hither, villain!\n",
      "My throat did use to play in your ear,\n",
      "A toy most like fancy: but you mean,\n",
      "My throat; and with one word did tell me\n",
      "That words in mine ear mean nothing.\n",
      "\n",
      "DUKE OF YORK:\n",
      "If they mean nothing, then let me speak.\n",
      "\n",
      "DUKE OF YORK:\n",
      "How can you speak to me now, when I am prepared.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response_model[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 50, 'do_sample': True}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('outputs/finetuned_shakespeare/tokenizer_config.json',\n",
       " 'outputs/finetuned_shakespeare/special_tokens_map.json',\n",
       " 'outputs/finetuned_shakespeare/vocab.json',\n",
       " 'outputs/finetuned_shakespeare/merges.txt',\n",
       " 'outputs/finetuned_shakespeare/added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"outputs/finetuned_shakespeare\")\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(\"outputs/finetuned_shakespeare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Let's load the pretrained model and get some inference to see if it is recorded correctly***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak,\n",
      "That we may with a simple mind hear you speak\n",
      "Some other word or other.\n",
      "\n",
      "Provost:\n",
      "I say he is in the prison, sir.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "If you be not, sir, we have reason\n",
      "To fear your voices: and therefore leave me to your good\n",
      "Will.\n",
      "\n",
      "Provost:\n",
      "It may please your lordship, sir, to have them,\n"
     ]
    }
   ],
   "source": [
    "loaded_model = GPT2LMHeadModel.from_pretrained(\"outputs/finetuned_shakespeare\")\n",
    "loaded_tokenizer = GPT2Tokenizer.from_pretrained(\"outputs/finetuned_shakespeare\")\n",
    "\n",
    "# Now you can use the loaded model and tokenizer as before\n",
    "loaded_generator = pipeline('text-generation', model=loaded_model, tokenizer=loaded_tokenizer)\n",
    "\n",
    "response_model = loaded_generator(\"Before we proceed any further, hear me speak,\", max_length=100, num_return_sequences=1)\n",
    "print(response_model[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "742a48b876b0f23f00b65fd2dcf3bf44f6d277fe7e9fa3363701a6dfee6c8cd9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.18 ('novus')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
