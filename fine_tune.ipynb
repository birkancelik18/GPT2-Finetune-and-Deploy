{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "# us the model\n",
    "from transformers import pipeline, set_seed\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Tokenize\n",
    "\n",
    "***Model*** The GPT2 Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n",
    "\n",
    "**Tokenizer**:  A tokenizer is in charge of preparing the inputs for a model.\n",
    "\n",
    "***PreTrainedTokenizer*** and ***PreTrainedTokenizerFast*** thus implement the main methods for using all the tokenizers:\n",
    "\n",
    "- Tokenizing (splitting strings in sub-word token strings), converting tokens strings to ids and back, and encoding/decoding (i.e., tokenizing and converting to integers).\n",
    "- Adding new tokens to the vocabulary in a way that is independent of the underlying structure (BPE, SentencePiece‚Ä¶).\n",
    "- Managing special tokens (like mask, beginning-of-sentence, etc.): adding them, assigning them to attributes in the tokenizer for easy access and making sure they are not split during tokenization. \n",
    "    \n",
    "Here is the link to [documentation](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)\n",
    "\n",
    "\n",
    "- GPT-2 Small ('gpt2'): 124 million parameters.\n",
    "- GPT-2 Medium ('gpt2-medium'): 345 million parameters.\n",
    "- GPT-2 Large ('gpt2-large'): 774 million parameters.\n",
    "- GPT-2 XL ('gpt2-xl'): 1.5 billion parameters.\n",
    "\n",
    "***Byte-Pair Encoding (BPE)*** vs ***Word Level Encoding***\n",
    "\n",
    "BPE emphasises more on subwords. Yet there might be issues with semantic information of those subwords. \n",
    "Word Level Encoding encodes word by word that preserves the semantic information more yet it has problems with unseen word encoding etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Data collators*** are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of train_dataset or eval_dataset.\n",
    "\n",
    "To be able to build batches, data collators may apply some processing (like padding). Some of them (like DataCollatorForLanguageModeling) also apply some random data augmentation (like random masking) on the formed batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/novus/miniconda3/envs/novus/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load your Shakespeare dataset\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"shakespeare_dataset.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "tokenizer (PreTrainedTokenizer or PreTrainedTokenizerFast) ‚Äî The tokenizer used for encoding the data.\n",
    "\n",
    "mlm (bool, optional, defaults to True) ‚Äî Whether or not to use masked language modeling.\n",
    " If set to False, the labels are the same as the inputs with the padding tokens ignored (by setting them to -100). \n",
    " Otherwise, the labels are -100 for non-masked tokens and the value to predict for the masked token.\n",
    "\n",
    "mlm_probability (float, optional, defaults to 0.15) ‚Äî The probability with which to (randomly) mask tokens in the input, when mlm is set to True.\n",
    "\n",
    "pad_to_multiple_of (int, optional) ‚Äî If set will pad the sequence to a multiple of the provided value.\n",
    "\n",
    "return_tensors (str) ‚Äî The type of Tensor to return. Allowable values are ‚Äúnp‚Äù, ‚Äúpt‚Äù and ‚Äútf‚Äù.\n",
    "\"\"\"\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # No masked language modeling for GPT-2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,    11,\n",
       "         3285,   502,  2740,    13,   198,   198,  3237,    25,   198,  5248,\n",
       "          461,    11,  2740,    13,   198,   198,  5962, 22307,    25,   198,\n",
       "         1639,   389,   477, 12939,  2138,   284,  4656,   621,   284,  1145,\n",
       "          680,    30,   198,   198,  3237,    25,   198,  4965,  5634,    13,\n",
       "        12939,    13,   198,   198,  5962, 22307,    25,   198,  5962,    11,\n",
       "          345,   760,   327,  1872,   385,  1526, 28599,   318,  4039,  4472,\n",
       "          284,   262,   661,    13,   198,   198,  3237,    25,   198,  1135,\n",
       "          760,   470,    11,   356,   760,   470,    13,   198,   198,  5962,\n",
       "        22307,    25,   198,  5756,   514,  1494,   683,    11,   290,   356,\n",
       "         1183,   423, 11676,   379,   674,   898,  2756,    13,   198,  3792,\n",
       "          470,   257, 15593,    30,   198,   198,  3237,    25,   198,  2949,\n",
       "          517,  3375,   319,   470,    26,  1309,   340,   307])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the example notebook from Hugging Face about finetuning a model. [Notebook Link](https://github.com/huggingface/notebooks/blob/main/examples/summarization.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/novus/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/novus/Desktop/model_finetune_deploy/wandb/run-20240207_155531-j2a5p8qi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/birkan/uncategorized/runs/j2a5p8qi' target=\"_blank\">azure-butterfly-1</a></strong> to <a href='https://wandb.ai/birkan/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/birkan/uncategorized' target=\"_blank\">https://wandb.ai/birkan/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/birkan/uncategorized/runs/j2a5p8qi' target=\"_blank\">https://wandb.ai/birkan/uncategorized/runs/j2a5p8qi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine-tuned-shakespeare\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,  # Adjust the number of epochs based on your needs\n",
    "    per_device_train_batch_size=4,  # Adjust batch size based on GPU memory\n",
    "    save_steps=10_000,  # Adjust save steps based on your needs\n",
    ")\n",
    "\n",
    "\n",
    "wandb.init(config=training_args)\n",
    "# Magic\n",
    "wandb.watch(model, log_freq=2)\n",
    "\n",
    "\n",
    "# Create Trainer and fine-tune the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() # report to wights to biases \n",
    "                # wandb\n",
    "# untrained modelin inital loss ne olur? \n",
    "# 3.7 neden y√ºksek dedik. neden ve ne olmalƒ±ydƒ±?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "response_model = generator(\"Before we proceed any further, hear me speak,\", max_length=200, num_return_sequences=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak,\n",
      "My lord: this is very hot of grief.\n",
      "\n",
      "LUCENTIO:\n",
      "Marry, God you hear it! how came ye to\n",
      "It was so; how it could be so fast-rotted\n",
      "In a day-sleeve, is too fast! Yet this my lord, you are apt to\n",
      "Do it; for there was a goodly harvest,\n",
      "And of the harvest of a goodly crop.\n",
      "\n",
      "BRUTUS:\n",
      "Nay.\n",
      "\n",
      "LUCENTIO:\n",
      "My Lord, the good of this time is too tedious\n",
      "To keep a simple succession of days.\n",
      "\n",
      "Provost:\n",
      "The time?\n",
      "\n",
      "VINCENTIO:\n",
      "I mean the day of our departure.\n",
      "\n",
      "NATHANUS:\n",
      "What!\n",
      "\n",
      "Provost:\n",
      "Not the goodly harvest, but the very fruit of our grace\n",
      "As we could well do it. By\n"
     ]
    }
   ],
   "source": [
    "print(response_model[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 50, 'do_sample': True}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('outputs/finetuned_shakespeare/tokenizer_config.json',\n",
       " 'outputs/finetuned_shakespeare/special_tokens_map.json',\n",
       " 'outputs/finetuned_shakespeare/vocab.json',\n",
       " 'outputs/finetuned_shakespeare/merges.txt',\n",
       " 'outputs/finetuned_shakespeare/added_tokens.json')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"outputs/finetuned_shakespeare\")\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(\"outputs/finetuned_shakespeare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Let's load the pretrained model and get some inference to see if it is recorded correctly***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = GPT2LMHeadModel.from_pretrained(\"outputs/finetuned_shakespeare\")\n",
    "loaded_tokenizer = GPT2Tokenizer.from_pretrained(\"outputs/finetuned_shakespeare\")\n",
    "\n",
    "# Now you can use the loaded model and tokenizer as before\n",
    "loaded_generator = pipeline('text-generation', model=loaded_model, tokenizer=loaded_tokenizer)\n",
    "\n",
    "response_model = loaded_generator(\"Before we proceed any further, hear me speak,\", max_length=50, num_return_sequences=1)\n",
    "print(response_model[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "742a48b876b0f23f00b65fd2dcf3bf44f6d277fe7e9fa3363701a6dfee6c8cd9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.18 ('novus')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
